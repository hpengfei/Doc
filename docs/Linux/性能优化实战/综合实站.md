## 案例一 为什么应用容器化后，启动慢了很多？

启动案例：

```
# docker run --name tomcat --cpus 0.1 -m 512M -p 8080:8080 -itd feisky/tomcat:8
```

启动容器后验证是否正常工作：

```
# curl http://localhost:8080
Hello, wolrd!
```

实际情况需要等待很久才能接收到容器的应答，在终端一查看容器日志信息：

```
# docker logs -f tomcat
...
ectory [/usr/local/tomcat/webapps/manager] has finished in [801] ms
13-Jun-2021 07:43:10.208 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
13-Jun-2021 07:43:11.302 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
13-Jun-2021 07:43:11.906 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 21097 ms
```

在终端二中执行下面的命令，多次尝试访问 Tomcat： 

```
# for ((i=0;i<30;i++)); do curl http://localhost:8080; sleep 1; done
Hello, wolrd!
curl: (52) Empty reply from server
curl: (7) Failed connect to localhost:8080; Connection refused
curl: (7) Failed connect to localhost:8080; Connection refused
...
```

过段时间发现容器已经退出了， 在终端一中，执行下面的命令，查看容器的状态： 

```
# docker ps -a
CONTAINER ID   IMAGE             COMMAND             CREATED          STATUS                       PORTS     NAMES
74e7238cee7d   feisky/tomcat:8   "catalina.sh run"   10 minutes ago   Exited (137) 7 minutes ago             tomcat
```

通过  docker inspect  查看容器的详细信息：

```
# docker inspect tomcat -f '{{json .State}}' |jq
{
  "Status": "exited",
  "Running": false,
  "Paused": false,
  "Restarting": false,
  "OOMKilled": true,
  "Dead": false,
  "Pid": 0,
  "ExitCode": 137,
  "Error": "",
  "StartedAt": "2021-06-13T07:42:36.003920432Z",
  "FinishedAt": "2021-06-13T07:45:32.033312826Z"
}
```

这次你可以看到，容器已经处于 exited 状态，OOMKilled 是 true，ExitCode 是 137。这其中，OOMKilled 表示容器被 OOM 杀死了。 

在终端中执行 dmesg 命令，查看系统日志，并定位 OOM 相关的日志： 

```
# dmesg |grep -C 10  --color 'out of memory'
[  617.662823]  [<ffffffff887908d1>] __do_page_fault+0x491/0x500
[  617.662824]  [<ffffffff88790975>] do_page_fault+0x35/0x90
[  617.662826]  [<ffffffff8878c778>] page_fault+0x28/0x30
[  617.662828] Task in /system.slice/docker-74e7238cee7d9c5b98198d0590be70ad1c6a1c3e1c01b1106f5444c41f39b397.scope killed as a result of limit of /system.slice/docker-74e7238cee7d9c5b98198d0590be70ad1c6a1c3e1c01b1106f5444c41f39b397.scope
[  617.662830] memory: usage 524260kB, limit 524288kB, failcnt 53068
[  617.662831] memory+swap: usage 1048576kB, limit 1048576kB, failcnt 13
[  617.662832] kmem: usage 0kB, limit 9007199254740988kB, failcnt 0
[  617.662833] Memory cgroup stats for /system.slice/docker-74e7238cee7d9c5b98198d0590be70ad1c6a1c3e1c01b1106f5444c41f39b397.scope: cache:28KB rss:524232KB rss_huge:16384KB mapped_file:0KB swap:524316KB inactive_anon:262296KB active_anon:261936KB inactive_file:8KB active_file:0KB unevictable:0KB
[  617.662839] [ pid ]   uid  tgid total_vm      rss nr_ptes swapents oom_score_adj name
[  617.662873] [19158]     0 19158  1141624   120444     606   144956             0 java
[  617.662879] Memory cgroup out of memory: Kill process 19292 (java) score 1014 or sacrifice child
[  617.662912] Killed process 19158 (java), UID 0, total-vm:4566496kB, anon-rss:466688kB, file-rss:15088kB, shmem-rss:0kB
[  618.753252] docker0: port 1(veth403b720) entered disabled state
[  618.754906] docker0: port 1(veth403b720) entered disabled state
[  618.756966] device veth403b720 left promiscuous mode
[  618.756974] docker0: port 1(veth403b720) entered disabled state
```

从 dmesg 的输出，你就可以看到很详细的 OOM 记录了。你应该可以看到下面几个关键点。 

1. 被杀死的是一个 java 进程。从内核调用栈上的 Memory cgroup out of memory可以看出，它是因为超过 cgroup 的内存限制，而被 OOM 杀死的。 
2. java 进程是在容器内运行的，而容器内存的使用量和限制都是 512M（524288kB）。目前使用量（524260kB）已经达到了限制，所以会导致 OOM。 
3. 被杀死的进程，PID 为 19158，虚拟内存为（total-vm:4566496kB），匿名内存为（anon-rss:466688kB），页内存为（15088kB）。

合这几点，可以看出，Tomcat 容器的内存主要用在了匿名内存中，而匿名内存，其实就是主动申请分配的堆内存。  堆内存的默认限制是物理内存的四分之一 ，执行下面的命令，重新启动 tomcat 容器，并调用 java 命令行来查看堆内存大小： 

```
# docker rm -f tomcat
# docker run --name tomcat --cpus 0.1 -m 512M -p 8080:8080 -itd feisky/tomcat:8
# docker exec tomcat java -XX:+PrintFlagsFinal -version | grep HeapSize
    uintx ErgoHeapSizeLimit                         = 0                                   {product}
    uintx HeapSizePerGCThread                       = 87241520                            {product}
    uintx InitialHeapSize                          := 127926272                           {product}
    uintx LargePageHeapSizeThreshold                = 134217728                           {product}
    uintx MaxHeapSize                              := 2046820352                          {product}
openjdk version "1.8.0_181"
OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-2~deb9u1-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
```

初始堆内存的大小（InitialHeapSize）是 127926272，而最大堆内存则是 2046820352（1.9G），这可比容器限制的 512 MB 大多了。 

之所以会这么大，其实是因为，容器内部看不到 Docker 为它设置的内存限制。虽然在启动容器时，我们通过 -m 512M 选项，给容器设置了 512M 的内存限制。但实际上，从容器内部看到的限制，却并不是 512M。 

```
# docker exec tomcat free -m
              total        used        free      shared  buff/cache   available
Mem:           7802         693        5928          11        1180        6840
Swap:          2047           0        2047
```

 解决方法是给 JVM 正确配置内存限制为 512M 就可以了 ，比如，你可以执行下面的命令，通过环境变量 JAVA_OPTS=’-Xmx512m -Xms512m’ ，把 JVM 的初始内存和最大内存都设为 512MB： 

```
# docker rm -f tomcat
# docker run --name tomcat --cpus 0.1 -m 512M -e JAVA_OPTS='-Xmx512m -Xms512m' -p 8080:8080 -itd feisky/tomcat:8
```

 接着，再切换到终端二中，重新在循环中执行 curl 命令，查看 Tomcat 的响应： 

```
# for ((i=0;i<30;i++)); do curl localhost:8080; sleep 1; done
Hello, wolrd!
Hello, wolrd!
...
```

已经能够响应，但是响应的时间太慢，下面排查tomcat响应过慢的问题。

继续运行循环执行curl命令，然后通过top查看系统状态：

```
top - 16:30:05 up 54 min,  3 users,  load average: 0.20, 0.06, 0.06
Tasks: 125 total,   1 running, 124 sleeping,   0 stopped,   0 zombie
%Cpu0  :  0.7 us,  0.0 sy,  0.0 ni, 99.0 id,  0.3 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  1.0 us,  0.0 sy,  0.0 ni, 98.6 id,  0.0 wa,  0.0 hi,  0.3 si,  0.0 st
KiB Mem :  7990056 total,  5670384 free,  1021616 used,  1298056 buff/cache
KiB Swap:  2097148 total,  2088188 free,     8960 used.  6604420 avail Mem

   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 20206 root      20   0 3038060 534204  15256 S   6.0  6.7   0:25.01 java
 20552 root      20   0       0      0      0 S   0.7  0.0   0:00.70 kworker/0:2
     9 root      20   0       0      0      0 S   0.3  0.0   0:06.50 rcu_sched
  4286 root      20   0       0      0      0 S   0.3  0.0   0:03.63 xfsaild/sda3
```

运行pidstat查看进程使用cpu详细信息：

```
Average:      UID      TGID       TID    %usr %system  %guest    %CPU   CPU  Command
Average:        0     20206         -    4.12    0.34    0.00    4.46     -  java
Average:        0         -     20206    0.00    0.00    0.00    0.00     -  |__java
Average:        0         -     20242    0.00    0.00    0.00    0.00     -  |__java
Average:        0         -     20243    0.76    0.05    0.00    0.81     -  |__java
Average:        0         -     20244    0.81    0.05    0.00    0.86     -  |__java
Average:        0         -     20245    0.88    0.10    0.00    0.98     -  |__java
Average:        0         -     20246    0.00    0.00    0.00    0.00     -  |__java
...
```

通过查看进程的详细信息，可以发现线程的%usr 使用率很低， 可能是因为 Docker 为容器设置了限制。  我们用 --cpus 0.1 ，为容器设置了 0.1 个 CPU 的限制，也就是 10% 的 CPU。 下面把 CPU 限制增大就可以了。比如，你可以执行下面的命令，将 CPU 限制增大到 1 ；然后再重启，并观察启动日志： 

```
# docker rm -f tomcat
# docker run --name tomcat --cpus 1 -m 512M -e JAVA_OPTS='-Xmx512m -Xms512m' -p 8080:8080 -itd feisky/tomcat:8
# docker logs -f tomcat
...
13-Jun-2021 09:36:40.711 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["http-nio-8080"]
13-Jun-2021 09:36:40.719 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler ["ajp-nio-8009"]
13-Jun-2021 09:36:40.730 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 1424 ms
```

 现在可以看到，Tomcat 的启动过程，只需要 1 秒就完成了，在使用curl访问可以发现响应的时间明显减少。

## 案例二服务器总是时不时丢包

启动案例：

```
docker run --name nginx --hostname nginx --privileged -p 80:80 -itd feisky/nginx:drop
```

 执行下面的 hping3 命令，验证 Nginx 是否可以正常访问：

```
# -c表示发送10个请求，-S表示使用TCP SYN，-p指定端口为80
# hping3 -c 10 -S -p 80 192.168.120.88
HPING 192.168.120.88 (ens33 192.168.120.88): S set, 40 headers + 0 data bytes
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=3 win=5120 rtt=1.4 ms
DUP! len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=3 win=5120 rtt=1005.9 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=0.8 ms
DUP! len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=1405.7 ms

--- 192.168.120.88 hping statistic ---
10 packets transmitted, 4 packets received, 60% packet loss
round-trip min/avg/max = 0.8/603.5/1405.7 ms
```

从 hping3 的输出中，我们可以发现，发送了 10 个请求包，却只收到了 4 个回复，60% 的包都丢了。再观察每个请求的 RTT 可以发现，RTT 也有非常大的波动变化，小的时候只有 0.8 ms，而大的时候则有 1405.7 ms。  根据这些输出，我们基本能判断，已经发生了丢包现象。 

###  查看容器内链路层 

```
# docker exec -it nginx bash
root@nginx:/# netstat -i
Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0       100       24      0      0 0             6      0      0      0 BMRU
lo       65536        0      0      0 0             0      0      0      0 LRU
```

输出中的 RX-OK、RX-ERR、RX-DRP、RX-OVR ，分别表示接收时的总包数、总错误数、进入 Ring Buffer 后因其他原因（如内存不足）导致的丢包数以及 Ring Buffer 溢出导致的丢包数。 

从这个输出中，我们没有发现任何错误，说明容器的虚拟网卡没有丢包。 所以接下来，我们还要检查一下 eth0 上是否配置了 tc 规则，并查看有没有丢包。我们继续容器终端中，执行下面的 tc 命令，不过这次注意添加 -s 选项，以输出统计信息： 

```
root@nginx:/# tc -s qdisc show dev eth0
qdisc netem 8001: root refcnt 2 limit 1000 loss 30%
 Sent 316 bytes 6 pkt (dropped 1, overlimits 0 requeues 0)
 backlog 0b 0p requeues 0
```

 从 tc 的输出中可以看到， eth0 上面配置了一个网络模拟排队规则（qdisc netem），并且配置了丢包率为 30%（loss 30%）。再看后面的统计信息，发送了 6 个包，但是丢了 1 个。 看来，应该就是这里，导致 Nginx 回复的响应包，被 netem 模块给丢了。 

执行下面的命令，删除 tc 中的 netem 模块： 

```
root@nginx:/# tc qdisc del dev eth0 root netem loss 30%
```

 切换到终端二中，重新执行刚才的 hping3 命令，看看现在还有没有问题： 

```
# hping3 -c 10 -S -p 80 192.168.120.88
HPING 192.168.120.88 (ens33 192.168.120.88): S set, 40 headers + 0 data bytes
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=0.8 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=2 win=5120 rtt=3.2 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=5 win=5120 rtt=2.7 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=2.2 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=9 win=5120 rtt=3.2 ms

--- 192.168.120.88 hping statistic ---
10 packets transmitted, 5 packets received, 50% packet loss
round-trip min/avg/max = 0.8/2.4/3.2 ms
```

 从 hping3 的输出中，我们可以看到，网络还是有 50% 的丢包； 

### 查看容器内网络层和传输层 

执行下面的 netstat -s 命令，就可以看到协议的收发汇总，以及错误信息了： 

```
root@nginx:/# netstat -s
Ip:
    Forwarding: 1  //开启转发
    30 total packets received  //总收包数
    0 forwarded    //转发包数
    0 incoming packets discarded   //接收丢包数
    18 incoming packets delivered  //接收的数据包数
    11 requests sent out   //发出的数据包数
Icmp:
    0 ICMP messages received   //收到的ICMP包数
    0 input ICMP message failed   //收到ICMP失败数
    ICMP input histogram:
    0 ICMP messages sent   //ICMP发送数
    0 ICMP messages failedvvv  //ICMP失败数
    ICMP output histogram:
Tcp:
    0 active connection openings   //主动连接数
    0 passive connection openings  //被动连接数
    7 failed connection attempts   //失败连接尝试数
    0 connection resets received   //接收的连接重置数
    0 connections established      //建立连接数
    18 segments received           //已接收报文数
    15 segments sent out           //已发送报文数
    3 segments retransmitted       //重传报文数
    0 bad segments received        //错误报文数
    0 resets sent                  //发出的连接重置数
Udp:
    0 packets received
...
UdpLite:
TcpExt:
    7 resets received for embryonic SYN_RECV sockets   //半连接重置数
    3 SYNs to LISTEN sockets dropped
    0 packet headers predicted
    TCPTimeouts: 4           //超时数
    TCPSynRetrans: 3         //SYN重传数
IpExt:
    InOctets: 1200
    OutOctets: 484
    InNoECTPkts: 30
```

 根据上面的输出，你可以看到，只有 TCP 协议发生了丢包和重传，分别是： 

7 次连接失败重试（7 failed connection attempts）

3 次重传（3 segments retransmitted）

7 次半连接重置（7 resets received for embryonic SYN_RECV sockets）

4 次超时（TCPTimeouts）

3 次 SYN 重传（TCPSynRetrans）

这个结果告诉我们，TCP 协议有多次超时和失败重试，并且主要错误是半连接重置。换句话说，主要的失败，都是三次握手失败。 

### 查看容器内 iptables 

首先我们要知道，除了网络层和传输层的各种协议，iptables 和内核的连接跟踪机制也可能会导致丢包。 我们先来看看连接跟踪， 要确认是不是连接跟踪导致的问题，其实只需要对比当前的连接跟踪数和最大连接跟踪数即可。 

不过，由于连接跟踪在 Linux 内核中是全局的（不属于网络命名空间），我们需要退出容器终端，回到主机中来查看。 你可以在容器终端中，执行 exit ；然后执行下面的命令，查看连接跟踪数： 

```
# sysctl net.netfilter.nf_conntrack_max
net.netfilter.nf_conntrack_max = 262144
# sysctl net.netfilter.nf_conntrack_count
net.netfilter.nf_conntrack_count = 6
```

从这儿你可以看到，连接跟踪数只有 6，而最大连接跟踪数则是 262144。显然，这里的丢包，不可能是连接跟踪导致的。 

iptables 规则，统一管理在一系列的表中，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。而每张表又可以包括一系列的链，用于对 iptables 规则进行分组管理。 

对于丢包问题来说，最大的可能就是被 filter 表中的规则给丢弃了。要弄清楚这一点，就需要我们确认，那些目标为 DROP 和 REJECT 等会弃包的规则，有没有被执行到。 可以直接查询 DROP 和 REJECT 等规则的统计信息，看看是否为 0。如果统计值不是 0 ，再把相关的规则拎出来进行分析。 

登录容器查看 filter 表统计数据：

```
# docker exec -it nginx bash
root@nginx:/# iptables -nvL
Chain INPUT (policy ACCEPT 18 packets, 720 bytes)
 pkts bytes target     prot opt in     out     source               destination
   12   480 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination

Chain OUTPUT (policy ACCEPT 11 packets, 484 bytes)
 pkts bytes target     prot opt in     out     source               destination
    4   176 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.29999999981
```

从 iptables 的输出中，你可以看到，两条 DROP 规则的统计数值不是 0，它们分别在 INPUT 和 OUTPUT 链中。这两条规则实际上是一样的，指的是使用 statistic 模块，进行随机 30% 的丢包。 

我们可以在容器终端中，执行下面的两条 iptables 命令，删除这两条 DROP 规则： 

```
iptables -t filter -D INPUT -m statistic --mode random --probability 0.30 -j DROP
iptables -t filter -D OUTPUT -m statistic --mode random --probability 0.30 -j DROP
```

切换到终端二中，重新执行刚才的 hping3 命令，看看现在是否正常： 

```
#  hping3 -c 10 -S -p 80 192.168.120.88
HPING 192.168.120.88 (ens33 192.168.120.88): S set, 40 headers + 0 data bytes
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=0 win=5120 rtt=0.5 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=1 win=5120 rtt=2.1 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=2 win=5120 rtt=0.5 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=3 win=5120 rtt=1.6 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=4 win=5120 rtt=0.5 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=5 win=5120 rtt=1.7 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=6 win=5120 rtt=1.2 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=7 win=5120 rtt=2.1 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=8 win=5120 rtt=2.6 ms
len=46 ip=192.168.120.88 ttl=63 DF id=0 sport=80 flags=SA seq=9 win=5120 rtt=2.0 ms

--- 192.168.120.88 hping statistic ---
10 packets transmitted, 10 packets received, 0% packet loss
round-trip min/avg/max = 0.5/1.5/2.6 ms
```

这次输出你可以看到，现在已经没有丢包了，并且延迟的波动变化也很小。看来，丢包问题应该已经解决了。 

执行如下的 curl 命令，检查 Nginx 对 HTTP 请求的响应： 

```
# curl --max-time 3 http://192.168.120.88
curl: (28) Operation timed out after 3004 milliseconds with 0 out of -1 bytes received
```

从 curl 的输出中，你可以发现，这次连接超时了。 下面使用tcpdump抓包分析。

### tcpdump

切换回终端一，在容器终端中，执行下面的 tcpdump 命令，抓取 80 端口的包： 

```
root@nginx:/# tcpdump -i eth0 -nn port 80
```

终端二中，再次执行前面的 curl 命令： 

```
# curl --max-time 3 http://192.168.120.88
```

 查看 tcpdump 的输出： 

```
11:38:11.859866 IP 192.168.120.74.40908 > 172.20.88.2.80: Flags [S], seq 1579751407, win 29200, options [mss 1460,sackOK,TS val 14283012 ecr 0,nop,wscale 7], length 0
11:38:11.859935 IP 172.20.88.2.80 > 192.168.120.74.40908: Flags [S.], seq 294029715, ack 1579751408, win 4880, options [mss 256,sackOK,TS val 14276059 ecr 14283012,nop,wscale 7], length 0
11:38:11.860615 IP 192.168.120.74.40908 > 172.20.88.2.80: Flags [.], ack 1, win 229, options [nop,nop,TS val 14283016 ecr 14276059], length 0
11:38:14.871292 IP 192.168.120.74.40908 > 172.20.88.2.80: Flags [F.], seq 79, ack 1, win 229, options [nop,nop,TS val 14286025 ecr 14276059], length 0
11:38:14.871505 IP 172.20.88.2.80 > 192.168.120.74.40908: Flags [.], ack 1, win 40, options [nop,nop,TS val 14279070 ecr 14283016,nop,nop,sack 1 {79:80}], length 0
```

 从 tcpdump 的输出中，我们就可以看到： 前三个包是正常的 TCP 三次握手，这没问题；  但第四个包却是在 3 秒以后了，并且还是客户端（VM2）发送过来的 FIN 包，也就说明，客户端的连接关闭了。并没有抓取到 curl 发来的 HTTP GET 请求。

 执行 netstat -i 命令，确认一下网卡有没有丢包问题： 

```
root@nginx:/# netstat -i
Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0       100     6900      0     56 0          3679      0      0      0 BMRU
lo       65536        0      0      0 0             0      0      0      0 LRU
```

由于 hping3 实际上只发送了 SYN 包；  而 curl 在发送 SYN 包后，还会发送 HTTP GET 请求。 从上面输出看到 eth0 的 MTU 只有 100，而以太网的 MTU 默认值是 1500，这个 100 就显得太小了。 

把容器 eth0 的 MTU 改成 1500： 

```
root@nginx:/# ifconfig eth0 mtu 1500
```

 再切换到终端二中，再次执行 curl 命令，确认问题是否真的解决了： 

```
# curl --max-time 3 http://192.168.120.88
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

