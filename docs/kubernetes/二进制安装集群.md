## 搭建环境简介

### 说明
此文档是按照这位老哥文档一步步搭建的，[传送门](https://jimmysong.io/kubernetes-handbook/practice/install-kubernetes-on-centos.html)。
### 集群简介

|IP|hostname|系统|角色|安装软件|
|:-:|:-:|:-:|:-:|:-:|
|192.168.122.71|C7-node1|7.4.1708|Node|docker,etcd,kubelet,flannel,kube-proxy
|192.168.122.72|C7-node2|7.4.1708|Node|docker,etcd,kubelet,flannel,kube-proxy
|192.168.122.73|C7-node3|7.4.1708|Node Master|docker,etcd,kubectl,flannel,kube-proxy,kube-apiserver,kube-controller-manager,kube-scheduler

### 安装前工作
关闭 `selinux`
```shell
# setenforce 0
```
时间同步
```shell
# /usr/sbin/ntpdate 1.centos.pool.ntp.org &>/dev/null
```
关闭防火墙
```shell
# systemctl stop firewalld
# systemctl disable firewalld
```
ssh免密登录
这是在 192.168.122.73 主机上生成公私钥，然后分发到集群中其他主机中去。
```shell
# ssh-keygen
# ssh-copy-id 192.168.122.72
# ssh-copy-id 192.168.122.73
```
主机名解析
三台机器都要添加各主机名的解析信息。
```shell
# tail -3 /etc/hosts
192.168.122.71  C7-node1
192.168.122.72  C7-node2
192.168.122.73  C7-node3
```
`docker` 安装
```shell
# yum install -y yum-utils device-mapper-persistent-data lvm2
# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# yum install -y docker-ce-18.09.8
```
三台机器均需要安装docker。
## 集群证书配置
### 安装命令
```shell
cd /root/
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
chmod +x cfssl_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssljson_linux-amd64
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson

wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl-certinfo_linux-amd64
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
```
### `CA` 证书配置
#### 创建 `CA` 配置文件
```shell
mkdir /root/ssl
cd /root/ssl
cfssl print-defaults config > config.json
cfssl print-defaults csr > csr.json
cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "87600h"
      }
    }
  }
}
EOF
```
#### 创建 `CA` 证书签名请求
```shell
cat > ca-csr.json <<EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ],
    "ca": {
       "expiry": "87600h"
    }
}
EOF
```
#### 生成 `CA` 证书和私钥
```shell
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
ls ca*
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
```
### 创建 `kubernetes` 证书
#### 创建证书配置文件
```shell
cat > kubernetes-csr.json <<EOF
{
    "CN": "kubernetes",
    "hosts": [
      "127.0.0.1",
      "192.168.122.71",
      "192.168.122.72",
      "192.168.122.73",
      "192.168.24.112",
      "10.102.25.15",
      "10.254.0.1",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "BeiJing",
            "L": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}
EOF
```
如果 `hosts` 字段不为空则需要指定授权使用该证书的 `IP` 或域名列表，由于该证书后续被 `etcd` 集群和 `kubernetes` `master` 集群使用，所以上面分别指定了 `etcd` 集群、`kubernetes master` 集群的主机 `IP` 和 `kubernetes` 服务的服务 `IP`（一般是 `kube-apiserver` 指定的 `service-cluster-ip-range` 网段的第一个IP，如 10.254.0.1）。
#### 生成 `kubernetes` 证书和私钥
```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
ls kubernetes*
kubernetes.csr  kubernetes-csr.json  kubernetes-key.pem  kubernetes.pem
```
### 创建 `admin` 证书
#### 创建证书配置文件
```shell
cat > admin-csr.json <<EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "system:masters",
      "OU": "System"
    }
  ]
}
EOF
```
#### 生成 `admin` 证书和私钥
```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
ls admin*
admin.csr  admin-csr.json  admin-key.pem  admin.pem
```
集群搭建后可以通过如下命名进行查看:
```shell
kubectl get clusterrolebinding cluster-admin -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2019-04-11T07:11:30Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: cluster-admin
  resourceVersion: "94"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin
  uid: 07e85ce5-5c29-11e9-bf14-000c29d63f6a
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:masters
```
### 创建 `kube-proxy` 证书
#### 创建证书配置文件
```shell
cat > kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
```
#### 生成 `kube-proxy` 证书和私钥
```shell
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
ls kube-proxy*
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
```
### 分发证书
将生成的证书和私钥文件拷贝到所有主机的的 `/etc/kubernetes/ssl` 目录下。
```shell
mkdir -p /etc/kubernetes/ssl
cd /root/ssl/
cp *.pem /etc/kubernetes/ssl
ssh root@192.168.122.72 "mkdir -p /etc/kubernetes/ssl"
ssh root@192.168.122.71 "mkdir -p /etc/kubernetes/ssl"
scp *.pem root@192.168.122.72:/etc/kubernetes/ssl
scp *.pem root@192.168.122.71:/etc/kubernetes/ssl
```
## 安装配置 `kubectl`
### 下载安装
[kubernetes-v1.14.0](https://storage.googleapis.com/kubernetes-release/release/v1.14.0/kubernetes-client-linux-amd64.tar.gz)
```shell
cd /root/
wget https://storage.googleapis.com/kubernetes-release/release/v1.14.0/kubernetes-client-linux-amd64.tar.gz
tar -xf kubernetes-client-linux-amd64.tar.gz
cd kubernetes/client/bin/
cp kubectl /usr/local/bin/
scp kubectl root@192.168.122.71:/usr/local/bin/
scp kubectl root@192.168.122.72:/usr/local/bin/
```
### 创建 `kubectl kubeconfig` 文件
```shell
# export KUBE_APISERVER="https://192.168.122.73:6443"
# kubectl config set-cluster kubernetes   --certificate-authority=/etc/kubernetes/ssl/ca.pem   --embed-certs=true   --server=${KUBE_APISERVER}
# kubectl config set-credentials admin   --client-certificate=/etc/kubernetes/ssl/admin.pem   --embed-certs=true   --client-key=/etc/kubernetes/ssl/admin-key.pem
# kubectl config set-context kubernetes   --cluster=kubernetes   --user=admin
# kubectl config use-context kubernetes
```
## 创建 `kubeconfig` 配置文件
### 创建 `TLS Bootstrapping Token`
```shell
# export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')
# cat > token.csv <<EOF
${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF
```
分发 `token.csv` 文件
```shell
cd /root/
cp token.csv /etc/kubernetes/
scp token.csv root@192.168.122.71:/etc/kubernetes/
scp token.csv root@192.168.122.72:/etc/kubernetes/
```
### 创建 `kubelet bootstrapping kubeconfig` 文件
```shell
# cd /root/
# export KUBE_APISERVER="https://192.168.122.73:6443"
# kubectl config set-cluster kubernetes   --certificate-authority=/etc/kubernetes/ssl/ca.pem   --embed-certs=true   --server=${KUBE_APISERVER}   --kubeconfig=bootstrap.kubeconfig
# kubectl config set-credentials kubelet-bootstrap   --token=${BOOTSTRAP_TOKEN}   --kubeconfig=bootstrap.kubeconfig
# kubectl config set-context default   --cluster=kubernetes   --user=kubelet-bootstrap   --kubeconfig=bootstrap.kubeconfig
# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
```
### 创建 `kube-proxy kubeconfig` 文件
```shell
# cd /root/
# export KUBE_APISERVER="https://192.168.122.73:6443"
# kubectl config set-cluster kubernetes   --certificate-authority=/etc/kubernetes/ssl/ca.pem   --embed-certs=true   --server=${KUBE_APISERVER}   --kubeconfig=kube-proxy.kubeconfig
# kubectl config set-credentials kube-proxy   --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem   --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem   --embed-certs=true   --kubeconfig=kube-proxy.kubeconfig
# kubectl config set-context default   --cluster=kubernetes   --user=kube-proxy   --kubeconfig=kube-proxy.kubeconfig
# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```
### 分发 `kubeconfig ` 文件
```shell
cd /root/
cp bootstrap.kubeconfig kube-proxy.kubeconfig /etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig  root@192.168.122.71:/etc/kubernetes/
scp bootstrap.kubeconfig kube-proxy.kubeconfig  root@192.168.122.72:/etc/kubernetes/
```
##  创建 `etcd` 集群
### `TLS` 认证文件
```shell
cd /root/ssl/
cp ca.pem kubernetes-key.pem kubernetes.pem /etc/kubernetes/ssl
```
文件前面已经拷贝过了，现在为防止出现证书问题重新在拷贝一次。
**kubernetes 证书的 `hosts` 字段列表中包含上面三台机器的 IP，否则后续证书校验会失败**
### 下载安装 `etcd`
[etcd-v3.3.10](https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz)
```shell
cd /root/
wget https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz
tar -xf etcd-v3.3.10-linux-amd64.tar.gz
cd etcd-v3.3.10-linux-amd64/
cp etcd* /usr/local/bin/
scp etcd etcdctl root@192.168.122.71:/usr/local/bin/
scp etcd etcdctl root@192.168.122.72:/usr/local/bin/
```
### `etcd` 配置启动
#### `systemd` 启动配置文件
192.168.122.73 主机上做如下的操作：
```shell
cat > /usr/lib/systemd/system/etcd.service <<EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name etcd-host0 \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://192.168.122.73:2380 \
  --listen-peer-urls https://192.168.122.73:2380 \
  --listen-client-urls https://192.168.122.73:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.122.73:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster etcd-host0=https://192.168.122.73:2380,etcd-host1=https://192.168.122.72:2380,etcd-host2=https://192.168.122.71:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
#### 集群配置文件
192.168.122.73 主机上做如下的操作：
```shell
mkdir /var/lib/etcd
mkdir /etc/etcd
ssh root@192.168.122.71 "mkdir {/var/lib/etcd,/etc/etcd}"
ssh root@192.168.122.72 "mkdir {/var/lib/etcd,/etc/etcd}"
```
```shell
cat > /etc/etcd/etcd.conf <<EOF
ETCD_NAME=infra1
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.122.73:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.122.73:2379"

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.122.73:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.122.73:2379"
EOF
```
#### 配置集群其余主机
192.168.122.71 主机上 `etcd` 的配置：
```shell
[root@C7-node1 ~] #
```
```shell
cat > /usr/lib/systemd/system/etcd.service <<EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name etcd-host2 \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://192.168.122.71:2380 \
  --listen-peer-urls https://192.168.122.71:2380 \
  --listen-client-urls https://192.168.122.71:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.122.71:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster etcd-host0=https://192.168.122.73:2380,etcd-host1=https://192.168.122.72:2380,etcd-host2=https://192.168.122.71:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
```shell
[root@C7-node1 ~]#
```
```shell
cat >/etc/etcd/etcd.conf <<EOF
ETCD_NAME=infra3
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.122.71:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.122.71:2379"

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.122.71:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.122.71:2379"
EOF
```
192.168.122.72 主机上 `etcd` 的配置：
```shell
[root@C7-node2 ~]#
```
```shell
cat > /usr/lib/systemd/system/etcd.service <<EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
EnvironmentFile=-/etc/etcd/etcd.conf
ExecStart=/usr/local/bin/etcd \
  --name etcd-host1 \
  --cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \
  --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \
  --initial-advertise-peer-urls https://192.168.122.72:2380 \
  --listen-peer-urls https://192.168.122.72:2380 \
  --listen-client-urls https://192.168.122.72:2379,http://127.0.0.1:2379 \
  --advertise-client-urls https://192.168.122.72:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster etcd-host0=https://192.168.122.73:2380,etcd-host1=https://192.168.122.72:2380,etcd-host2=https://192.168.122.71:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
```shell
cat >/etc/etcd/etcd.conf <<EOF
ETCD_NAME=infra2
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.122.72:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.122.72:2379"

#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.122.72:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.122.72:2379"
EOF
```
#### 启动服务
三台机器都执行如下命令：
```shell
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
systemctl status etcd
```
#### 验证服务
```shell
[root@C7-node3 ~]# etcdctl   --ca-file=/etc/kubernetes/ssl/ca.pem   --cert-file=/etc/kubernetes/ssl/kubernetes.pem   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem   cluster-health
member 1895c0eff8c94336 is healthy: got healthy result from https://192.168.122.71:2379
member 8366c2b8cc7c2e78 is healthy: got healthy result from https://192.168.122.73:2379
member 898af78b7470cd73 is healthy: got healthy result from https://192.168.122.72:2379
cluster is healthy
```
## 部署 `master` 节点
`kubernetes` 包含的组件有`kube-scheduler`、`kube-controller-manager` 和 `kube-apiserver` ,同时只能有一个进程处于工作状态，如果有多个需要选举一个leader。
### 下载安装
[kubernetes-v1.14.0](https://storage.googleapis.com/kubernetes-release/release/v1.14.0/kubernetes-server-linux-amd64.tar.gz)
```shell
tar -xf kubernetes-server-linux-amd64.tar.gz
cd kubernetes/server/bin/
cp {kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/
```
### 配置启动 `kube-apiserver`
192.168.122.73 主机做如下操作：
```shell
cat >/usr/lib/systemd/system/kube-apiserver.service <<EOF
[Unit]
Description=Kubernetes API Service
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
After=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/apiserver
ExecStart=/usr/local/bin/kube-apiserver \$KUBE_LOGTOSTDERR \$KUBE_LOG_LEVEL \$KUBE_ETCD_SERVERS \$KUBE_API_ADDRESS \$KUBE_API_PORT \$KUBELET_PORT \$KUBE_ALLOW_PRIV \$KUBE_SERVICE_ADDRESSES \$KUBE_ADMISSION_CONTROL \$KUBE_API_ARGS
Restart=on-failure
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
```shell
cat >/etc/kubernetes/config <<EOF
###
# kubernetes system config
#
# The following values are used to configure various aspects of all
# kubernetes services, including
#
#   kube-apiserver.service
#   kube-controller-manager.service
#   kube-scheduler.service
#   kubelet.service
#   kube-proxy.service
# logging to stderr means we get it in the systemd journal
KUBE_LOGTOSTDERR="--logtostderr=true"

# journal message level, 0 is debug
KUBE_LOG_LEVEL="--v=0"

# Should this cluster be allowed to run privileged docker containers
KUBE_ALLOW_PRIV="--allow-privileged=true"

# How the controller-manager, scheduler, and proxy find the apiserver
KUBE_MASTER="--master=http://192.168.122.73:8080"
EOF
```
```shell
cat > /etc/kubernetes/apiserver <<EOF
###
## kubernetes system config
##
## The following values are used to configure the kube-apiserver
##
#
## The address on the local server to listen to.
#KUBE_API_ADDRESS="--insecure-bind-address=test-001.jimmysong.io"
KUBE_API_ADDRESS="--advertise-address=192.168.122.73 --bind-address=192.168.122.73 --insecure-bind-address=192.168.122.73"
#
## The port on the local server to listen on.
#KUBE_API_PORT="--port=8080"
#
## Port minions listen on
#KUBELET_PORT="--kubelet-port=10250"
#
## Comma separated list of nodes in the etcd cluster
KUBE_ETCD_SERVERS="--etcd-servers=https://192.168.122.71:2379,https://192.168.122.72:2379,https://192.168.122.73:2379"
#
## Address range to use for services
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"
#
## default admission control policies
KUBE_ADMISSION_CONTROL="--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota"
#
## Add your own!
KUBE_API_ARGS="--authorization-mode=Node,RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --enable-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h"
EOF
```
```shell
systemctl daemon-reload
systemctl enable kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
```
### 配置启动 `kube-controller-manager`
192.168.122.73 主机做如下操作：
```shell
cat >/usr/lib/systemd/system/kube-controller-manager.service <<EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/controller-manager
ExecStart=/usr/local/bin/kube-controller-manager \$KUBE_LOGTOSTDERR \$KUBE_LOG_LEVEL \$KUBE_MASTER \$KUBE_CONTROLLER_MANAGER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
```shell
cat >/etc/kubernetes/controller-manager <<EOF
###
# The following values are used to configure the kubernetes controller-manager

# defaults from config and apiserver should be adequate

# Add your own!
KUBE_CONTROLLER_MANAGER_ARGS="--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true"
EOF
```
```shell
systemctl daemon-reload
systemctl enable kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
```
启动组件后可以通过 `kubectl get componentstatuses` 命令查看各组件的状态：
```shell
# kubectl get componentstatuses
NAME                 STATUS      MESSAGE                                                                                     ERROR
scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Healthy     ok                                                                                          
etcd-0               Healthy     {"health":"true"}                                                                           
etcd-1               Healthy     {"health":"true"}                                                                           
etcd-2               Healthy     {"health":"true"}  
```
### 配置启动 `kube-scheduler`
192.168.122.73 主机做如下操作：
```shell
cat > /usr/lib/systemd/system/kube-scheduler.service <<EOF
[Unit]
Description=Kubernetes Scheduler Plugin
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/scheduler
ExecStart=/usr/local/bin/kube-scheduler \$KUBE_LOGTOSTDERR \$KUBE_LOG_LEVEL \$KUBE_MASTER \$KUBE_SCHEDULER_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
```shell
cat >/etc/kubernetes/scheduler <<EOF
###
# kubernetes scheduler config

# default config should be adequate

# Add your own!
KUBE_SCHEDULER_ARGS="--leader-elect=true --address=127.0.0.1"
EOF
```
```shell
systemctl daemon-reload
systemctl enable kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler
```
### 验证 `master` 节点功能
```shell
[root@C7-node3 ~]# kubectl get componentstatuses                     
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-2               Healthy   {"health":"true"}   
etcd-0               Healthy   {"health":"true"}   
etcd-1               Healthy   {"health":"true"}   
```
## 安装 `flannel` 网络插件
### 安装
三台机器全部执行安装：
```shell
# yum install -y flannel
```
### 配置启动
192.168.122.73 主机做如下操作：
```shell
cat >/usr/lib/systemd/system/flanneld.service<<EOF
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
After=network-online.target
Wants=network-online.target
After=etcd.service
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/flanneld
EnvironmentFile=-/etc/sysconfig/docker-network
ExecStart=/usr/bin/flanneld-start \$FLANNEL_OPTIONS
ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker
Restart=on-failure

[Install]
WantedBy=multi-user.target
WantedBy=docker.service
EOF
```
```shell
cat >/etc/sysconfig/flanneld<<EOF
# Flanneld configuration options  

# etcd url location.  Point this to the server where etcd runs
#FLANNEL_ETCD_ENDPOINTS="http://127.0.0.1:2379"
FLANNEL_ETCD_ENDPOINTS="https://192.168.122.71:2379,https://192.168.122.72:2379,https://192.168.122.73:2379"

# etcd config key.  This is the configuration key that flannel queries
# For address range assignment
#FLANNEL_ETCD_PREFIX="/atomic.io/network"
FLANNEL_ETCD_PREFIX="/kube-centos/network"

# Any additional options that you want to pass
#FLANNEL_OPTIONS=""
FLANNEL_OPTIONS="-etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem -etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem"
EOF
```
将配置文件分别拷贝到剩余两台主机：
```shell
# scp /etc/sysconfig/flanneld root@192.168.122.71:/etc/sysconfig/
# scp /etc/sysconfig/flanneld root@192.168.122.72:/etc/sysconfig/
# scp /usr/lib/systemd/system/flanneld.service root@192.168.122.71:/usr/lib/systemd/system/
# scp /usr/lib/systemd/system/flanneld.service root@192.168.122.72:/usr/lib/systemd/system/
# etcdctl --endpoints=https://192.168.122.71:2379,https://192.168.122.72:2379,https://192.168.122.73:2379   --ca-file=/etc/kubernetes/ssl/ca.pem   --cert-file=/etc/kubernetes/ssl/kubernetes.pem   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem   mkdir /kube-centos/network
# etcdctl --endpoints=https://192.168.122.71:2379,https://192.168.122.72:2379,https://192.168.122.73:2379   --ca-file=/etc/kubernetes/ssl/ca.pem   --cert-file=/etc/kubernetes/ssl/kubernetes.pem   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem   mk /kube-centos/network/config '{"Network":"172.30.0.0/16","SubnetLen":24,"Backend":{"Type":"vxlan"}}'
```
三台机器需要执行启动命令：
```shell
systemctl daemon-reload
systemctl enable flanneld
systemctl start flanneld
systemctl status flanneld
```
### 验证
```shell
[root@C7-node3 ~]# etcdctl --endpoints=${ETCD_ENDPOINTS}   --ca-file=/etc/kubernetes/ssl/ca.pem   --cert-file=/etc/kubernetes/ssl/kubernetes.pem   --key-file=/etc/kubernetes/ssl/kubernetes-key.pem   ls /kube-centos/network/subnets
/kube-centos/network/subnets/172.30.89.0-24
/kube-centos/network/subnets/172.30.21.0-24
/kube-centos/network/subnets/172.30.11.0-24
```
## 部署 `node` 节点
### 配置 `docker`
```shell
cat >/usr/lib/systemd/system/docker.service<<EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
[Service]
Type=notify
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd -H fd:// --containerd=/run/containerd/containerd.sock
ExecReload=/bin/kill -s HUP \$MAINPID
TimeoutSec=0
RestartSec=2
Restart=always
StartLimitBurst=3
StartLimitInterval=60s
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
EnvironmentFile=-/run/flannel/docker
EnvironmentFile=-/run/flannel/subnet.env
TasksMax=infinity
Delegate=yes
KillMode=process
[Install]
WantedBy=multi-user.target
EOF
```
三台node机器修改 `docker` 的启动配置重新载入文件，然后在重启服务。
```shell
scp /usr/lib/systemd/system/docker.service 192.168.122.71:/usr/lib/systemd/system/
scp /usr/lib/systemd/system/docker.service 192.168.122.72:/usr/lib/systemd/system/
```
```shell
systemctl daemon-reload
systemctl enable docker
systemctl restart docker
systemctl status docker
```
### 安装配置 `kubelet`
192.168.122.73 主机做如下操作：
#### 权限配置
```shell
cd /etc/kubernetes
kubectl create clusterrolebinding kubelet-bootstrap   --clusterrole=system:node-bootstrapper   --user=kubelet-bootstrap
kubectl create clusterrolebinding kubelet-nodes   --clusterrole=system:node   --group=system:nodes
```
#### 安装 `kubelet` 和 `kube-proxy`
由于前面下载的 [kubernetes-v1.14.0](https://storage.googleapis.com/kubernetes-release/release/v1.14.0/kubernetes-server-linux-amd64.tar.gz) 中包含，所以只要将其拷贝至 `/usr/local/bin/` 目录下即可。
```shell
cp kubernetes/server/bin/{kubelet,kube-proxy} /usr/local/bin/
scp kubernetes/server/bin/{kubelet,kube-proxy} root@192.168.122.71:/usr/local/bin/
scp kubernetes/server/bin/{kubelet,kube-proxy} root@192.168.122.72:/usr/local/bin/
```
#### 创建 `systemd` 启动文件
```shell
cat >/usr/lib/systemd/system/kubelet.service <<EOF
[Unit]
Description=Kubernetes Kubelet Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/kubelet
ExecStart=/usr/local/bin/kubelet \$KUBE_LOGTOSTDERR  \$KUBE_LOG_LEVEL \$KUBELET_API_SERVER \$KUBELET_ADDRESS \$KUBELET_PORT \$KUBELET_HOSTNAME \$KUBE_ALLOW_PRIV \$KUBELET_POD_INFRA_CONTAINER \$KUBELET_ARGS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF
```
```shell
mkdir /var/lib/kubelet
ssh root@192.168.122.71 "mkdir /var/lib/kubelet"
ssh root@192.168.122.72 "mkdir /var/lib/kubelet"
scp /usr/lib/systemd/system/kubelet.service root@192.168.122.71:/usr/lib/systemd/system/
scp /usr/lib/systemd/system/kubelet.service root@192.168.122.72:/usr/lib/systemd/system/
```
#### 创建 `kubelet` 配置文件
```shell
cat >/etc/kubernetes/kubelet <<EOF
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=192.168.122.73"
#
## The port for the info server to serve on
#KUBELET_PORT="--port=10250"
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=192.168.122.73"
#
## location of the api-server
## COMMENT THIS ON KUBERNETES 1.8+
#KUBELET_API_SERVER="--api-servers=http://192.168.122.73:8080"
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=gcr.azk8s.cn/google-containers/pause:3.1"
#
## Add your own!
KUBELET_ARGS="--cgroup-driver=systemd --cluster-dns=10.254.0.2 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local --hairpin-mode promiscuous-bridge --serialize-image-pulls=false --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"
EOF
```
#### 拉取镜像
```shell
docker pull gcr.azk8s.cn/google-containers/pause:3.1
ssh root@192.168.122.72 "docker pull gcr.azk8s.cn/google-containers/pause:3.1"
ssh root@192.168.122.71 "docker pull gcr.azk8s.cn/google-containers/pause:3.1"
```
#### 配置剩下Node节点
192.168.122.71 主机的 `kubelet` 配置文件如下：
```shell
cat >/etc/kubernetes/kubelet <<EOF
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=192.168.122.71"
#
## The port for the info server to serve on
#KUBELET_PORT="--port=10250"
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=192.168.122.71"
#
## location of the api-server
## COMMENT THIS ON KUBERNETES 1.8+
#KUBELET_API_SERVER="--api-servers=http://192.168.122.71:8080"
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=gcr.azk8s.cn/google-containers/pause:3.1"
#
## Add your own!
KUBELET_ARGS="--cgroup-driver=systemd --cluster-dns=10.254.0.2 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local --hairpin-mode promiscuous-bridge --serialize-image-pulls=false --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"
EOF
```
192.168.122.72 主机的 `kubelet` 配置文件如下：
```shell
cat >/etc/kubernetes/kubelet <<EOF
###
## kubernetes kubelet (minion) config
#
## The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)
KUBELET_ADDRESS="--address=192.168.122.72"
#
## The port for the info server to serve on
#KUBELET_PORT="--port=10250"
#
## You may leave this blank to use the actual hostname
KUBELET_HOSTNAME="--hostname-override=192.168.122.72"
#
## location of the api-server
## COMMENT THIS ON KUBERNETES 1.8+
#KUBELET_API_SERVER="--api-servers=http://192.168.122.72:8080"
#
## pod infrastructure container
KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=gcr.azk8s.cn/google-containers/pause:3.1"
#
## Add your own!
KUBELET_ARGS="--cgroup-driver=systemd --cluster-dns=10.254.0.2 --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local --hairpin-mode promiscuous-bridge --serialize-image-pulls=false --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice"
EOF
```
#### 启动 `kublet`
三台 `Node` 节点均执行下面的启动命令：
```shell
systemctl daemon-reload
systemctl enable kubelet
systemctl start kubelet
systemctl status kubelet
```
#### 通过 `kublet` 的 `TLS` 证书请求
```shell
[root@node3 ~]# kubectl get nodes
No resources found.
[root@node3 ~]# kubectl get csr  
NAME                                                   AGE   REQUESTOR           CONDITION
node-csr-03US9Hu-MUiifLSOCGHamjeG0xMKHazYSNqz9DqC53I   23m   kubelet-bootstrap   Pending
node-csr-OXm7n49sMkq_0B_rXAe92qF2MJrMSOdM-T0eC73wFrk   24m   kubelet-bootstrap   Pending
node-csr-jU7RP-ZUNhTPW8LnJgMPFF4N1ab_R6NpU0GnV22fKkc   24m   kubelet-bootstrap   Pending
[root@node3 ~]# kubectl certificate approve node-csr-03US9Hu-MUiifLSOCGHamjeG0xMKHazYSNqz9DqC53I
certificatesigningrequest.certificates.k8s.io/node-csr-03US9Hu-MUiifLSOCGHamjeG0xMKHazYSNqz9DqC53I approved
[root@node3 ~]# kubectl get csr
NAME                                                   AGE   REQUESTOR           CONDITION
node-csr-03US9Hu-MUiifLSOCGHamjeG0xMKHazYSNqz9DqC53I   28m   kubelet-bootstrap   Approved,Issued
node-csr-OXm7n49sMkq_0B_rXAe92qF2MJrMSOdM-T0eC73wFrk   29m   kubelet-bootstrap   Pending
node-csr-jU7RP-ZUNhTPW8LnJgMPFF4N1ab_R6NpU0GnV22fKkc   29m   kubelet-bootstrap   Pending
[root@node3 ~]# kubectl get nodes
NAME             STATUS   ROLES    AGE   VERSION
192.168.122.72   Ready    <none>   10s   v1.14.0
[root@node3 ~]# kubectl certificate approve node-csr-OXm7n49sMkq_0B_rXAe92qF2MJrMSOdM-T0eC73wFrk
certificatesigningrequest.certificates.k8s.io/node-csr-OXm7n49sMkq_0B_rXAe92qF2MJrMSOdM-T0eC73wFrk approved
[root@node3 ~]# kubectl certificate approve node-csr-jU7RP-ZUNhTPW8LnJgMPFF4N1ab_R6NpU0GnV22fKkc
certificatesigningrequest.certificates.k8s.io/node-csr-jU7RP-ZUNhTPW8LnJgMPFF4N1ab_R6NpU0GnV22fKkc approved
[root@node3 ~]# kubectl get nodes
NAME             STATUS   ROLES    AGE    VERSION
192.168.122.71   Ready    <none>   76s    v1.14.0
192.168.122.72   Ready    <none>   2m7s   v1.14.0
192.168.122.73   Ready    <none>   69s    v1.14.0
[root@node3 ~]# kubectl get csr
NAME                                                   AGE   REQUESTOR           CONDITION
node-csr-03US9Hu-MUiifLSOCGHamjeG0xMKHazYSNqz9DqC53I   30m   kubelet-bootstrap   Approved,Issued
node-csr-OXm7n49sMkq_0B_rXAe92qF2MJrMSOdM-T0eC73wFrk   31m   kubelet-bootstrap   Approved,Issued
node-csr-jU7RP-ZUNhTPW8LnJgMPFF4N1ab_R6NpU0GnV22fKkc   31m   kubelet-bootstrap   Approved,Issued
```
### 配置 `kube-proxy`
192.168.122.73 主机上做如下的操作：
#### 安装 `conntrack`
```shell
yum install -y conntrack-tools
ssh root@192.168.122.71 "yum install -y conntrack-tools"
ssh root@192.168.122.72 "yum install -y conntrack-tools"
```
#### 创建 `systemd` 启动文件
```shell
cat >/usr/lib/systemd/system/kube-proxy.service <<EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
EnvironmentFile=-/etc/kubernetes/config
EnvironmentFile=-/etc/kubernetes/proxy
ExecStart=/usr/local/bin/kube-proxy \$KUBE_LOGTOSTDERR \$KUBE_LOG_LEVEL \$KUBE_MASTER \$KUBE_PROXY_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```
```shell
scp /usr/lib/systemd/system/kube-proxy.service root@192.168.122.71:/usr/lib/systemd/system/
scp /usr/lib/systemd/system/kube-proxy.service root@192.168.122.72:/usr/lib/systemd/system/
```
#### 创建 `kube-proxy` 配置文件
```shell
cat >/etc/kubernetes/proxy <<EOF
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS="--bind-address=192.168.122.73 --hostname-override=192.168.122.73 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16"
EOF
```
#### 配置其余 `Node` 节点
192.168.122.71 主机做如下操作：
```shell
cat >/etc/kubernetes/proxy <<EOF
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS="--bind-address=192.168.122.71 --hostname-override=192.168.122.71 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16"
EOF
```
192.168.122.72 主机做如下操作：
```shell
cat >/etc/kubernetes/proxy <<EOF
###
# kubernetes proxy config

# default config should be adequate

# Add your own!
KUBE_PROXY_ARGS="--bind-address=192.168.122.72 --hostname-override=192.168.122.72 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16"
EOF
```
#### 启动 `kube-proxy` 服务
三台 `Node` 节点做如下操作：
```shell
systemctl daemon-reload
systemctl enable kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
```
## 验证集群
### 创建 `Nginx` 集群
```shell
[root@node3 ~]# kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
[root@node3 ~]# kubectl get pods
NAME                     READY   STATUS              RESTARTS   AGE
nginx-65f88748fd-vkpn6   0/1     ContainerCreating   0          11s
[root@node3 ~]# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-65f88748fd-vkpn6   1/1     Running   0          20s
[root@node3 ~]# kubectl scale deployment nginx --replicas=3
deployment.extensions/nginx scaled
[root@node3 ~]# kubectl get pods
NAME                     READY   STATUS              RESTARTS   AGE
nginx-65f88748fd-8cbxq   0/1     ContainerCreating   0          7s
nginx-65f88748fd-8vxd8   0/1     ContainerCreating   0          7s
nginx-65f88748fd-vkpn6   1/1     Running             0          39s
[root@node3 ~]# kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
nginx-65f88748fd-8cbxq   1/1     Running   0          29s
nginx-65f88748fd-8vxd8   1/1     Running   0          29s
nginx-65f88748fd-vkpn6   1/1     Running   0          61s
[root@node3 ~]# kubectl expose deployment nginx --port=8080 --target-port=80 --type=NodePort  
service/nginx exposed
[root@node3 ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes   ClusterIP   10.254.0.1     <none>        443/TCP          24h
nginx        NodePort    10.254.36.40   <none>        8080:32237/TCP   6s
[root@node3 ~]# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP           NODE             NOMINATED NODE   READINESS GATES
nginx-65f88748fd-8cbxq   1/1     Running   0          2m55s   172.17.0.2   192.168.122.73   <none>           <none>
nginx-65f88748fd-8vxd8   1/1     Running   0          2m55s   172.17.0.2   192.168.122.71   <none>           <none>
nginx-65f88748fd-vkpn6   1/1     Running   0          3m27s   172.17.0.2   192.168.122.72   <none>           <none>
[root@node3 ~]# kubectl describe svc nginx
Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP:                       10.254.36.40
Port:                     <unset>  8080/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  32237/TCP
Endpoints:                172.17.0.2:80,172.17.0.2:80,172.17.0.2:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
```
### 验证
```shell
[root@node3 ~]# curl -I http://10.254.36.40:8080
HTTP/1.1 200 OK
Server: nginx/1.15.12
Date: Fri, 19 Apr 2019 09:14:12 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
Connection: keep-alive
ETag: "5cb5d3c3-264"
Accept-Ranges: bytes
[root@localhost ~]# curl  -I http://192.168.122.73:32237
HTTP/1.1 200 OK
Server: nginx/1.15.12
Date: Fri, 19 Apr 2019 09:15:26 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
Connection: keep-alive
ETag: "5cb5d3c3-264"
Accept-Ranges: bytes

[root@localhost ~]# curl  -I http://192.168.122.71:32237
HTTP/1.1 200 OK
Server: nginx/1.15.12
Date: Fri, 19 Apr 2019 09:16:15 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
Connection: keep-alive
ETag: "5cb5d3c3-264"
Accept-Ranges: bytes

[root@localhost ~]# curl  -I http://192.168.122.72:32237
HTTP/1.1 200 OK
Server: nginx/1.15.12
Date: Fri, 19 Apr 2019 09:16:19 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
Connection: keep-alive
ETag: "5cb5d3c3-264"
Accept-Ranges: bytes
```

